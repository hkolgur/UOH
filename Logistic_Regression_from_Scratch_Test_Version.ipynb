{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hari- Logistic Regression from Scratch Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hkolgur/UOH/blob/main/Logistic_Regression_from_Scratch_Test_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOoMmn6xpOkJ"
      },
      "source": [
        "# Implement Logistic Regression from scratch\n",
        "\n",
        "In this assignment, you will implement Logistic Regression with L1 regularization from scratch and predict the labels of the test data. You will then verify the correctness of the your implementation using multiple \"grader\" functions/cells (provided by us) which will match your implmentation.\n",
        "\n",
        "The grader functions would help you validate the correctness of your code. \n",
        "\n",
        "Please submit the final Colab notebook in the classroom ONLY after you have verified your code using the grader functions/cells.\n",
        "\n",
        "\n",
        "**NOTE: DO NOT change the \"grader\" functions or code snippets written by us.Please add your code in the suggested locations.**\n",
        "\n",
        "Ethics Code:\n",
        "1. You are welcome to read up online resources to implement the code. \n",
        "2. You can also discuss with your classmates on the implmentation over Slack.\n",
        "3. But, the code you write and submit should be yours ONLY. Your code will be compared against other stduents' code and online code snippets to check for plagiarism. If your code is found to be plagiarised, you will be awarded zero-marks for all assignments, which have a 10% weightage in the final marks for this course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAlENnFPpQ3Y",
        "outputId": "26507ed2-27a2-4272-a274-8f785150cdfd"
      },
      "source": [
        "# Code to mount google drive in case you are loading the data from your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK7B3kn4dONk",
        "outputId": "5eac08fd-8902-4cb6-e864-c2ffdc178725"
      },
      "source": [
        "ls"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mMyDrive\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "KMnahnQEpfQb",
        "outputId": "fea8dc9d-8b79-4c2e-d677-32fdc08a50a5"
      },
      "source": [
        "# Loading data from csv file\n",
        "import pandas as pd\n",
        "data_path = '/gdrive/My Drive/UOH/LogisticRegression_from_scratch/logistic_regression_assignment_data.csv'\n",
        "\n",
        "df = pd.read_csv(data_path)\n",
        "df"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>henman hopes ended in dubai third seed tim hen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>wilkinson fit to face edinburgh england captai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1012</th>\n",
              "      <td>0</td>\n",
              "      <td>wall street cool to ebay s profit shares in on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1013</th>\n",
              "      <td>0</td>\n",
              "      <td>ban on forced retirement under 65 employers wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1014</th>\n",
              "      <td>1</td>\n",
              "      <td>time to get tough on friendlies  for an intern...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1015</th>\n",
              "      <td>0</td>\n",
              "      <td>christmas shoppers flock to tills shops all ov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1016</th>\n",
              "      <td>0</td>\n",
              "      <td>bush budget seeks deep cutbacks president bush...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1017 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      category                                               text\n",
              "0            0  worldcom boss  left books alone  former worldc...\n",
              "1            1  tigers wary of farrell  gamble  leicester say ...\n",
              "2            1  yeading face newcastle in fa cup premiership s...\n",
              "3            1  henman hopes ended in dubai third seed tim hen...\n",
              "4            1  wilkinson fit to face edinburgh england captai...\n",
              "...        ...                                                ...\n",
              "1012         0  wall street cool to ebay s profit shares in on...\n",
              "1013         0  ban on forced retirement under 65 employers wi...\n",
              "1014         1  time to get tough on friendlies  for an intern...\n",
              "1015         0  christmas shoppers flock to tills shops all ov...\n",
              "1016         0  bush budget seeks deep cutbacks president bush...\n",
              "\n",
              "[1017 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTQOrw6JCu5D"
      },
      "source": [
        "#### **Note:** Here class-0 is of category \"business\" and class-1 is of category \"sport\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0_a8GvRqwzj",
        "outputId": "e53fc86c-f691-483a-83c6-c4bc9cd8555f"
      },
      "source": [
        "# Data Overiview\n",
        "df['category'].value_counts()"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    509\n",
              "0    508\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW3eJmVbubx4"
      },
      "source": [
        "### Creating Train and Test Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5iXh7PusnyJ",
        "outputId": "f35eaa70-e6f9-46cf-8985-246125cc9355"
      },
      "source": [
        "# Splitting the data into train and test\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "text = df['text']\n",
        "category = df['category']\n",
        "train_text, test_text, train_category, test_category = train_test_split(text, category, random_state=42, stratify=category, test_size=0.01)\n",
        "\n",
        "print(\"Shape of Train_Text = \", train_text.shape)\n",
        "print(\"Shape of Test_Text = \", test_text.shape)\n",
        "print(\"Shape of Train_Category = \", train_category.shape)\n",
        "print(\"Shape of Train_Category = \", test_category.shape)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of Train_Text =  (1006,)\n",
            "Shape of Test_Text =  (11,)\n",
            "Shape of Train_Category =  (1006,)\n",
            "Shape of Train_Category =  (11,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6JlCeWPvNPS"
      },
      "source": [
        "## Custom Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQRHOpf1vO69"
      },
      "source": [
        "### Instructions:\n",
        "\n",
        "  1. Read in the train_data.\n",
        "  2. Vectorize train_data and test_data using sklearns built in tfidf vectorizer.\n",
        "  3. Ignore unigrams and make use of both **bigrams & trigrams** and also limit the **max features** to **2000** and **minimum document frequency** to **10**.\n",
        "  4. After the tfidf vectors are generated as mentioned above, next task is to column standardize your data.\n",
        "  5. We want you to write in comments in your code, the reason you think for standardizing the data in the above step.\n",
        "  6. You can use sklearn StandardScaler to column standardize your data.\n",
        "  7. Write a function to initialise your weights & bias. And then run its corresponding grader function.\n",
        "  8. Write a custom function to calculate sigmoid of a value. And then run its corresponding grader function to cross check your implementation of sigmoid function.\n",
        "  9. Write a custom function to compute the total loss as the sum of log loss and l1 regularization loss based on true labels and predicted labels and weights. And you can crosscheck your implementation with its corresponding grader.\n",
        "  10. Write a function to compute gradients for your weights and bias terms, which you have to make use of in updating your weights and bias while training your model.\n",
        "  11. Implement a custom train function of logistic regression, wherein you take in the following inputs:\n",
        "        * **X_train** which will be your vectorized text data\n",
        "        * **y_train** which are the labels for your train data\n",
        "        * **alpha** = 0.0001 which is the regularization factor (λ) \n",
        "        * **eta0** = 0.0001 which will be the learning rate   \n",
        "        * **tolerance** = 0.001\n",
        "        \n",
        "  12. In the custom train function you should make use of a custom SGD function to update the weights and bias terms for **each** of your inputs. \n",
        "  13. The custom SGD implemented in the above train function for updating the weights and bias terms should run for many epochs until the difference in loss between two consecutive epochs is less than tolerance.\n",
        "\n",
        "  14. Here one epoch means a complete iteration of your entire train data.\n",
        "  15. Your train function should return the follwing:\n",
        "        * the number of epochs it took to complete the training\n",
        "        * train loss for all epochs\n",
        "        * the values for final weights and bias terms.\n",
        "        \n",
        "  16. Now run the grader function to check whether the weights and bias obtained from your custom implementation are close enough to that of sklearns implementation.\n",
        "  17. Next write a custom predict function which takes in as input the weights and bias values that you computed in your train function, and also takes in the test standardized data as input to predict its labels.\n",
        "  18. Now run the grader function to check the accuracy of your predictions.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kb2V8ZXM-6M"
      },
      "source": [
        "###Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3Zm11bHNGBi"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import linear_model\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hfDStz1LvEe"
      },
      "source": [
        "### 1. Vectorize train data and test data using sklearn tf-idf in the below cell\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoiLZawBMI2B",
        "outputId": "9e0ab58d-82a7-4b8f-b87f-884b858b9fe5"
      },
      "source": [
        "'''vectorize train and test data using TF-IDF and store them in train_vectors and test_vectors respectively'''\n",
        "model=TfidfVectorizer(lowercase=True,ngram_range=(2,3),max_features=2000,min_df=10)\n",
        "train_vectors = model.fit_transform(train_text.tolist())\n",
        "test_vectors = model.transform(test_text.tolist())\n",
        "train_vectors.shape, test_vectors.shape"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1006, 2000), (11, 2000))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0igVcuTSN4pS"
      },
      "source": [
        "###2. Column standardize the train and test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zqOC3wx8LWF"
      },
      "source": [
        "What is the reasoning for column standardizing the data?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4jJ4xm58UPZ"
      },
      "source": [
        "Answer: In Logistic Regression we compute the distance of the point from the plane that separate positive and negative points . To avoid impact of scale on distances we compute , we need to do column standardization ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9vQGIN_N8u2",
        "outputId": "6c8ef86d-25c4-47bc-d5ef-650ca4097c1a"
      },
      "source": [
        "'''column standardize the train and test data and store them in train_vectors_stand and test_vectors_stand'''\n",
        "sclar=StandardScaler(with_mean=False)\n",
        "train_vectors_stand =sclar.fit_transform(train_vectors)\n",
        "test_vectors_stand =sclar.transform(test_vectors)\n",
        "train_vectors_stand.shape, test_vectors_stand.shape"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1006, 2000), (11, 2000))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq5Ne3WIiUwV",
        "outputId": "e6db8d34-8156-4c32-bf7c-0d35923c70ea"
      },
      "source": [
        "l1=[max(e) for e in train_vectors.toarray().tolist()]\n",
        "max(l1)"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8952512562635134"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5H1j1bo5e2-"
      },
      "source": [
        "### Grader Function - 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_wIbcOj3JrV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "414dd3d6-9c06-4280-9c0e-21c03ad262f9"
      },
      "source": [
        "# Grader function to check the initialization of your weights and bias terms.\n",
        "def initialize_weights_bias(d):\n",
        "  weights=np.zeros(d)\n",
        "  #weights=[0]*d # cannot be list because we cannot do .T/dot on vectors\n",
        "  bias=0\n",
        "  return weights,bias\n",
        "\n",
        "def grader_weights_bias(w,b):\n",
        "  assert((len(w)==2000) and b==0)\n",
        "  return True\n",
        "\n",
        "dim = 2000\n",
        "w,b = initialize_weights_bias(dim)\n",
        "grader_1 = grader_weights_bias(w,b)\n",
        "print(\"Grader_1 Status : \", grader_1)"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Grader_1 Status :  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLGMaOFE6JDF"
      },
      "source": [
        "### 4. Custom function to calculate sigmoid of a value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfP2Z7gv3JpX"
      },
      "source": [
        "import math \n",
        "def custom_sigmoid(z):\n",
        "    ''' In this function, we will return sigmoid of z'''\n",
        "    \n",
        "    # Compute sigmoid(z) and return its value.\n",
        "    # Write your code below.\n",
        "    #print(\"z value is :\",z)\n",
        "    sigmoid=1/(1+math.exp(-z))\n",
        "\n",
        "    return sigmoid"
      ],
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmoPhaX16pJ2"
      },
      "source": [
        "### Grader Function - 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G14xZAg_6x6C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d244765-c0ed-4232-8b91-0729ed7413a0"
      },
      "source": [
        "# Grader function to check the implementaiton of sigmoid function\n",
        "\n",
        "def grader_sigmoid(z):\n",
        "  val = custom_sigmoid(z)\n",
        "  assert(val==0.8807970779778823)\n",
        "  return True\n",
        "\n",
        "grader_2 = grader_sigmoid(2)\n",
        "print(\"Grader_2 Status : \", grader_2)"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Grader_2 Status :  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVTM6TpQ6JTS"
      },
      "source": [
        "### 5.  Custom function to compute loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5B5oKpf4h0n"
      },
      "source": [
        "$logloss = -1*\\frac{1}{n}\\Sigma_{for each Y_{true},Y_{pred}}(Y_{true}log10(Y_{pred})+(1-Y_{true})log10(1-Y_{pred}))$ \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMDWzUoYPQxC"
      },
      "source": [
        "(yi)* - log(p(yi)) + (1-yi) * - 1og(p(1-yi)  - Each Term"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnS1uSrV6CAk"
      },
      "source": [
        "$L1 loss = \\Sigma_{for each w}(|w|)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azcvxO3u6lNK"
      },
      "source": [
        "$total loss = logloss + alpha*L1loss$<br>\n",
        "Where alphas is the regularization parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41ush74x8ueR"
      },
      "source": [
        "def custom_loss(y_true, y_pred, alpha, we):\n",
        "    '''In this function, we will compute total loss which is [(logloss) + (alpha * L1regularization loss)] '''\n",
        "    \n",
        "    # Write your code below.\n",
        "    loss_lg=0\n",
        "    l1_loss=0\n",
        "\n",
        "    for i,pred in enumerate(y_pred):\n",
        "      loss_lg=loss_lg +((1-y_true[i])*-math.log10(1-pred)+y_true[i]*-math.log10(pred))\n",
        "      #print((1-y_true[i])*-math.log10(1-pred)+y_true[i]*-math.log10(pred))\n",
        "    #print(\"Losss\",loss_lg)\n",
        "    logloss= loss_lg/len(y_pred)\n",
        "    #log_loss =\n",
        "    #print(\"Log Loss{} and avg log loss{}:\".format(loss_lg,logloss))\n",
        "    for wt in we:\n",
        "      #print(\"wt:{} and abs(wt){}\".format(wt,abs(wt)))\n",
        "      l1_loss=l1_loss+ abs(wt)\n",
        "    #print(\"L1 loss:\",l1_loss)\n",
        "    #print(\"Alpha and L1 is:\",alpha*l1_loss)\n",
        "    total_loss = logloss + alpha*l1_loss \n",
        "    #print(\"Total Loss:\",total_loss)\n",
        "\n",
        "    return total_loss"
      ],
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYPtH_bM6obu"
      },
      "source": [
        "### Grader Function - 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeNjSysS829s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f8f50e-35d6-40b0-aec5-542672f8d57d"
      },
      "source": [
        "# Grader function to check the implementaiton of logloss\n",
        "\n",
        "def grader_loss():\n",
        "  true_values = [1,1,0,1,0]\n",
        "  pred_values = [0.9,0.8,0.1,0.8,0.2]\n",
        "  wts= np.array([0.1]*10)\n",
        "  #print(\"W is: \",wts)\n",
        "  alpha= 0.0001\n",
        "  loss = custom_loss(true_values, pred_values,alpha,wts)\n",
        "  #print(\"Total Loss:\",loss)\n",
        "  assert(loss==(0.07644900402910389+0.0001*10*0.1))\n",
        "  return True\n",
        "\n",
        "\n",
        "grader_3 = grader_loss()\n",
        "print(\"Grader_3 Status : \", grader_3)"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Grader_3 Status :  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQGM1j2B6Jbf"
      },
      "source": [
        "### 6. Custom function to updated weights and bias terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlqoG2CBMAjI"
      },
      "source": [
        "Use the below formula to compute gradient of your weight and bias terms <br>\n",
        "Loss term Li for a single example is given as below: \n",
        "<br>\n",
        "<br>\n",
        "\n",
        "$Li= -(Y_{i}log10(𝝈_{i})-(1-Y_{i})log10(1-𝝈_{i}) + \\frac{alpha}{N}(sum(|w|))\n",
        "$ <br>\n",
        "<br>\n",
        "$Where: 𝝈_{i} = σ(w^{T} x_i+b) $ <br>\n",
        "<br>\n",
        "And: L1 regularization = $\\frac{alpha}{N}(sum(|w|)) $ <br>\n",
        "Alpha: It is the Regularization parameter <br>\n",
        "N : number of training examples<br>\n",
        "σ : sigmoid function <br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "$dLi/dw= -Y_{i}x_{i}(1-𝝈_{i}) + (1-Y_{i})x_{i}𝝈_{i} + \\frac{w + (1e-5)}{|w + (1e-5)|}  $<br>\n",
        "NOTE THAT: 1e-5 used in numerator and denominator to avoid division error <br>\n",
        "\n",
        "$dLi/db= -Y_{i}(1-𝝈_{i}) + (1-Y_{i})𝝈_{i}$<br>\n",
        "<br>\n",
        "<br>\n",
        "Hence,<br>\n",
        "$dLi/dw= dw = (𝝈_{i} -Y_{i})x_{i} + \\frac{alpha}{N}\\frac{w + (1e-5)}{|w + (1e-5)|} $<br>\n",
        "1e-5 used in numerator and denominator to avoid division error <br>\n",
        "$dLi/db =  db = 𝝈_{i}-Y_{i}$\n",
        "<br>\n",
        "<br>\n",
        "!!NOTE: USE NEGATIVE GRADIENT WHILE UPDATING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UBBkZNJM2Fg"
      },
      "source": [
        "### 6a. Custom function to compute Gradient of loss function wrt weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hREi7bQxM8h-"
      },
      "source": [
        "def gradient_dw(x, y, w, b, alpha, N):\n",
        "    '''In this function, we will compute the gardient w.r.t. w '''\n",
        "    \n",
        "    # Write your code below.\n",
        "    # print(\"w.shape\",w.shape)\n",
        "    # print(\"x shape\",x.shape)\n",
        "    # print(\"b is: \",b)\n",
        "    # print(\"dot product of w and x: \",np.dot(x.toarray(),w))\n",
        "    # print(\"custom_sigmoid(w.T*x+b):\",custom_sigmoid(np.dot(x.toarray(),w)+b))\n",
        "    dw= (custom_sigmoid(np.dot(x.toarray(),w.T)+b) -y)*x+ (alpha/N)*((w+.00001)/abs(w+.00001))\n",
        "    # print(\"dw shape:\",dw.shape)\n",
        "    #print(\"dw value:\",dw)\n",
        "    return dw\n"
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2C-hOu0ruBs",
        "outputId": "e09cc293-0633-4259-ec57-26d7636383f8"
      },
      "source": [
        "a1=np.array([2,2,2,2,2])\n",
        "x1=np.array([1,1,1,1,1])\n",
        "b1=np.array([4,4,4,4,4])\n",
        "\n",
        "# print(custom_sigmoid(a1.T*b1+x1) + (a1+.00001)/abs(b1+.00001))\n",
        "# print(custom_sigmoid(np.dot(a1,b1)+x1) + (a1+.00001)/abs(b1+.00001))\n",
        "print(a1.T*b1)\n",
        "print(np.dot(a1,b1))\n",
        "k=np.zeros(5)\n",
        "k.shape\n",
        "k=k.reshape(1,5)\n",
        "k.shape\n",
        "\n",
        "wl=np.array([2,-2,5])\n",
        "print(wl.shape)\n",
        "xl=np.array([4,-4,3])\n",
        "print(x1.shape)\n",
        "print(\"prod\",wl*xl)\n",
        "print(\"dot:\",np.dot(wl,xl))\n"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[8 8 8 8 8]\n",
            "40\n",
            "(3,)\n",
            "(5,)\n",
            "prod [ 8  8 15]\n",
            "dot: 31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyxGabPPM_lL"
      },
      "source": [
        "### 6b.  Custom function to compute Gradient of loss function wrt bias term:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIMxwuBAND0a"
      },
      "source": [
        "def gradient_db(x, y, w, b):\n",
        "    '''In this function, we will compute the gardient w.r.t. b '''\n",
        "    \n",
        "    # Write your code below.\n",
        "    \n",
        "    db=(custom_sigmoid(np.dot(x.toarray(),w.T)+b) -y)\n",
        "\n",
        "    return db"
      ],
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SIRUHESkc7n",
        "outputId": "d64b1699-0cd1-4b30-fb2f-29fca4940e63"
      },
      "source": [
        "\n",
        "train_category.values.shape\n",
        "train_vectors_stand.shape\n",
        "train_vectors_stand[0]\n",
        "train_vectors_stand[0].shape[1]"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 258
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg_hI4q7wVP-",
        "outputId": "03338cd3-aaa6-43c0-a3a3-b7dcf0611f2b"
      },
      "source": [
        "wk=np.ones(2000)\n",
        "print(wk.shape)\n",
        "print(train_vectors_stand[0].shape)\n",
        "np.dot(train_vectors_stand[0].toarray(),wk)"
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000,)\n",
            "(1, 2000)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([330.74641329])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA6UuRWA-Pvf"
      },
      "source": [
        "###6c. Custom function to train logistic regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgdgYdyRX06U"
      },
      "source": [
        "$w^{(t+1)}← w^{(t)}- eta0*(dw^{(t)}) $<br>\n",
        "$b^{(t+1)}←b^{(t)} - eta0*(db^{(t)}) $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44nod2Gu-ZWq"
      },
      "source": [
        "#(train_vectors_stand, train_category.values, 0.0001,0.0001,0.001) \n",
        "def custom_train(X_train, y_train,alpha, eta0,tolerance):\n",
        "  \"\"\"\n",
        "  In this function we will compute optimal values for weights and bias terms on\n",
        "  the train data. \n",
        "\n",
        "  Here eta0 is the learning rate and alpha is the regularization term.\n",
        "  \"\"\"\n",
        "  train_loss=[]\n",
        "  y_pred=[]\n",
        "  # Implement the code as follows:\n",
        "\n",
        "  # 1. Initalize the weights (call the initialize_weights(X_train[0]) function)\n",
        "  # 2. Repeat For many epochs until condition \"e\"  fails\n",
        "          # a) for every data point(X_train,y_train)\n",
        "                # compute gradient w.r.to w (call the gradient_dw() function)\n",
        "                # compute gradient w.r.to b (call the gradient_db() function)\n",
        "                # update w, b using the above eqns\n",
        "          # b) predict the output of x_train[for all data points in X_train] using w,b\n",
        "          # c) compute the loss between predicted and actual values (call the loss function)\n",
        "          # d) store all the train loss values in a list\n",
        "          # e) Compare previous loss and current loss, if the difference between loss is not more than or equal to the tolerance, stop the process and return w,b\n",
        "\n",
        "  # 3. Return the values of weights, bias, train_loss and num_epochs \n",
        "  \n",
        "  w1,b1=initialize_weights_bias(X_train[0].shape[1])\n",
        "  #w1=w1.reshape(1,2000)\n",
        "  #print(w1.shape)\n",
        "  num_epochs=0\n",
        "  train_loss.clear()\n",
        "  while(True):\n",
        "    #print(\"Infinate\")\n",
        "    y_pred.clear()\n",
        "    num_epochs += 1\n",
        "    for i,x in enumerate(X_train):\n",
        "      #w1,b1=initialize_weights_bias(x.shape[1])\n",
        "      #print(\"x shape and w1.shape before grad\",x.shape,w1.shape)\n",
        "      #print(\"Before dw\")\n",
        "      #print(\"w1 shape\",w1.shape)\n",
        "      grad_dw=gradient_dw(x,y_train[i],w1,b1,alpha,y_train.shape[0])\n",
        "      #print(\"grad_dw\",grad_dw)\n",
        "      grad_db=gradient_db(x,y_train[i],w1,b1)\n",
        "      #print(grad_dw.shape)\n",
        "      #print(\"grad of db\",grad_db)\n",
        "      #print(grad_db.shape)\n",
        "      w1=w1-eta0*grad_dw\n",
        "      #print(\"grad of db\",grad_db)\n",
        "      b1=b1-eta0*grad_db\n",
        "      #print(\"w1\",w1.shape)\n",
        "      #print(\"x\",x.shape)\n",
        "      #print(\"B1\",b1)\n",
        "      #print(w1.T.reshape(w1.T.shape[0],1)*x+b1)\n",
        "      #print(\"Dot prod:,\",np.dot(x.toarray(),w1.T))\n",
        "      y_pred.append(custom_sigmoid(np.dot(x.toarray(),w1.T)+b1)) \n",
        "    print(\"Len of Y_pred\",len(y_pred))\n",
        "    #print(\"weightttt\",w1)\n",
        "    #print(\"Biasss\",b1)\n",
        "    print(\"Len of  y_train\",len(y_train))\n",
        "    loss_tr=custom_loss(y_train, y_pred, alpha,w)\n",
        "    print(\"loss_tr\",loss_tr)\n",
        "    train_loss.append(loss_tr)\n",
        "    print(\"train_loss array\",train_loss)\n",
        "    print(\"num_epochs\",num_epochs)\n",
        "    print(\"len of train_loss is:\",len(train_loss))\n",
        "    print(\"Loss Difference:\",train_loss[num_epochs-2]-train_loss[num_epochs-1])\n",
        "    print(\"Tollerence is: \",tolerance)\n",
        "    if(num_epochs>1 and (train_loss[num_epochs-2]-train_loss[num_epochs-1])<tolerance):\n",
        "      break\n",
        "  return w1,b1,train_loss,num_epochs\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h46YOxcpZzgG",
        "outputId": "f29f4426-c8f8-4138-c1b6-b47935d15c3c"
      },
      "source": [
        "train_loss"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.21071039693014837,\n",
              " 0.12789779912888735,\n",
              " 0.09236735360552176,\n",
              " 0.0728411306682425,\n",
              " 0.06044738113186946,\n",
              " 0.05183881727143685,\n",
              " 0.045483684031713684,\n",
              " 0.040582614034499566,\n",
              " 0.03667718176613776,\n",
              " 0.03348518626855716,\n",
              " 0.03082306523867378,\n",
              " 0.028566038415654433,\n",
              " 0.026626173843664724,\n",
              " 0.02493962250406359,\n",
              " 0.02345883556380749,\n",
              " 0.02214762967527783,\n",
              " 0.02097795190410939,\n",
              " 0.019927696107006242,\n",
              " 0.018979191514232704]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKKUVLml6pGj"
      },
      "source": [
        "### Grader Function - 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAgSNFOU3JhN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ac52259-f7e1-4a1d-b4fd-36c64b386278"
      },
      "source": [
        "def grader_weights_bias():\n",
        "  # fitting sklearn SGD classifier\n",
        "  clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l1', tol=1e-3, learning_rate='constant')\n",
        "  clf.fit(train_vectors_stand,train_category.values)\n",
        "  model_coef= clf.coef_[0]\n",
        "  print(\"Co-efficents from Model are:\",model_coef)\n",
        "  # fitting custom train with same learning rate, regularization and tolerance as of sklearn\n",
        "  w,b,_,epoch = custom_train(train_vectors_stand, train_category.values, 0.0001,0.0001,0.001) \n",
        "  print(\"Co-efficents from Custom model are:\",w)\n",
        "  # checking whether the weights and bias returned by both the implementations are closer\n",
        "  assert((not (w-model_coef>0.02).any())==True)\n",
        "  assert(not (b-clf.intercept_>0.02)==True)\n",
        "  \n",
        "  return True\n",
        "\n",
        "grader_4 = grader_weights_bias()\n",
        "print(\"Grader_4 Status : \", grader_4)"
      ],
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Co-efficents from Model are: [-0.02692747 -0.02015433  0.02406057 ...  0.02065403  0.02668987\n",
            "  0.02397516]\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.21071039693014837\n",
            "train_loss array [0.21071039693014837]\n",
            "num_epochs 1\n",
            "len of train_loss is: 1\n",
            "Loss Difference: 0.0\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.12789779912888735\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735]\n",
            "num_epochs 2\n",
            "len of train_loss is: 2\n",
            "Loss Difference: 0.08281259780126102\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.09236735360552176\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176]\n",
            "num_epochs 3\n",
            "len of train_loss is: 3\n",
            "Loss Difference: 0.035530445523365595\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.0728411306682425\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425]\n",
            "num_epochs 4\n",
            "len of train_loss is: 4\n",
            "Loss Difference: 0.01952622293727925\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.06044738113186946\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946]\n",
            "num_epochs 5\n",
            "len of train_loss is: 5\n",
            "Loss Difference: 0.012393749536373046\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.05183881727143685\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685]\n",
            "num_epochs 6\n",
            "len of train_loss is: 6\n",
            "Loss Difference: 0.008608563860432614\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.045483684031713684\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684]\n",
            "num_epochs 7\n",
            "len of train_loss is: 7\n",
            "Loss Difference: 0.006355133239723164\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.040582614034499566\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566]\n",
            "num_epochs 8\n",
            "len of train_loss is: 8\n",
            "Loss Difference: 0.004901069997214118\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.03667718176613776\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566, 0.03667718176613776]\n",
            "num_epochs 9\n",
            "len of train_loss is: 9\n",
            "Loss Difference: 0.0039054322683618053\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.03348518626855716\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566, 0.03667718176613776, 0.03348518626855716]\n",
            "num_epochs 10\n",
            "len of train_loss is: 10\n",
            "Loss Difference: 0.003191995497580598\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.03082306523867378\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566, 0.03667718176613776, 0.03348518626855716, 0.03082306523867378]\n",
            "num_epochs 11\n",
            "len of train_loss is: 11\n",
            "Loss Difference: 0.002662121029883384\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.028566038415654433\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566, 0.03667718176613776, 0.03348518626855716, 0.03082306523867378, 0.028566038415654433]\n",
            "num_epochs 12\n",
            "len of train_loss is: 12\n",
            "Loss Difference: 0.002257026823019346\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.026626173843664724\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566, 0.03667718176613776, 0.03348518626855716, 0.03082306523867378, 0.028566038415654433, 0.026626173843664724]\n",
            "num_epochs 13\n",
            "len of train_loss is: 13\n",
            "Loss Difference: 0.0019398645719897085\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.02493962250406359\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566, 0.03667718176613776, 0.03348518626855716, 0.03082306523867378, 0.028566038415654433, 0.026626173843664724, 0.02493962250406359]\n",
            "num_epochs 14\n",
            "len of train_loss is: 14\n",
            "Loss Difference: 0.0016865513396011333\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.02345883556380749\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566, 0.03667718176613776, 0.03348518626855716, 0.03082306523867378, 0.028566038415654433, 0.026626173843664724, 0.02493962250406359, 0.02345883556380749]\n",
            "num_epochs 15\n",
            "len of train_loss is: 15\n",
            "Loss Difference: 0.0014807869402561\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.02214762967527783\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566, 0.03667718176613776, 0.03348518626855716, 0.03082306523867378, 0.028566038415654433, 0.026626173843664724, 0.02493962250406359, 0.02345883556380749, 0.02214762967527783]\n",
            "num_epochs 16\n",
            "len of train_loss is: 16\n",
            "Loss Difference: 0.0013112058885296624\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.02097795190410939\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566, 0.03667718176613776, 0.03348518626855716, 0.03082306523867378, 0.028566038415654433, 0.026626173843664724, 0.02493962250406359, 0.02345883556380749, 0.02214762967527783, 0.02097795190410939]\n",
            "num_epochs 17\n",
            "len of train_loss is: 17\n",
            "Loss Difference: 0.001169677771168439\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.019927696107006242\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566, 0.03667718176613776, 0.03348518626855716, 0.03082306523867378, 0.028566038415654433, 0.026626173843664724, 0.02493962250406359, 0.02345883556380749, 0.02214762967527783, 0.02097795190410939, 0.019927696107006242]\n",
            "num_epochs 18\n",
            "len of train_loss is: 18\n",
            "Loss Difference: 0.0010502557971031473\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.018979191514232704\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566, 0.03667718176613776, 0.03348518626855716, 0.03082306523867378, 0.028566038415654433, 0.026626173843664724, 0.02493962250406359, 0.02345883556380749, 0.02214762967527783, 0.02097795190410939, 0.019927696107006242, 0.018979191514232704]\n",
            "num_epochs 19\n",
            "len of train_loss is: 19\n",
            "Loss Difference: 0.0009485045927735379\n",
            "Tollerence is:  0.001\n",
            "Co-efficents from Custom model are: [[-0.02331322 -0.0180137   0.02007448 ...  0.01832593  0.02251139\n",
            "   0.02023576]]\n",
            "Grader_4 Status :  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHrNKvsemncP"
      },
      "source": [
        "### 7. Plot the train loss with x as epoch number and y as train loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ_7XOwzmvvi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "77eb49b1-b631-4cf0-851d-ed5dfb0e001b"
      },
      "source": [
        "# plotting graph for epoch vs loss for train and test data\n",
        "\n",
        "w,b,train_loss,epochs = custom_train(train_vectors_stand, train_category.values, 0.0001,0.0001,0.001)\n",
        "plt.plot(range(epochs),train_loss,label='train curve') \n",
        "plt.title('epoch vs loss')\n",
        "plt.legend()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.21071039693014837\n",
            "train_loss array [0.21071039693014837]\n",
            "num_epochs 1\n",
            "len of train_loss is: 1\n",
            "Loss Difference: 0.0\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.12789779912888735\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735]\n",
            "num_epochs 2\n",
            "len of train_loss is: 2\n",
            "Loss Difference: 0.08281259780126102\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.09236735360552176\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176]\n",
            "num_epochs 3\n",
            "len of train_loss is: 3\n",
            "Loss Difference: 0.035530445523365595\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.0728411306682425\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425]\n",
            "num_epochs 4\n",
            "len of train_loss is: 4\n",
            "Loss Difference: 0.01952622293727925\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.06044738113186946\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946]\n",
            "num_epochs 5\n",
            "len of train_loss is: 5\n",
            "Loss Difference: 0.012393749536373046\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.05183881727143685\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685]\n",
            "num_epochs 6\n",
            "len of train_loss is: 6\n",
            "Loss Difference: 0.008608563860432614\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.045483684031713684\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684]\n",
            "num_epochs 7\n",
            "len of train_loss is: 7\n",
            "Loss Difference: 0.006355133239723164\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.040582614034499566\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566]\n",
            "num_epochs 8\n",
            "len of train_loss is: 8\n",
            "Loss Difference: 0.004901069997214118\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.03667718176613776\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566, 0.03667718176613776]\n",
            "num_epochs 9\n",
            "len of train_loss is: 9\n",
            "Loss Difference: 0.0039054322683618053\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.03348518626855716\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566, 0.03667718176613776, 0.03348518626855716]\n",
            "num_epochs 10\n",
            "len of train_loss is: 10\n",
            "Loss Difference: 0.003191995497580598\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.03082306523867378\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566, 0.03667718176613776, 0.03348518626855716, 0.03082306523867378]\n",
            "num_epochs 11\n",
            "len of train_loss is: 11\n",
            "Loss Difference: 0.002662121029883384\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.028566038415654433\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566, 0.03667718176613776, 0.03348518626855716, 0.03082306523867378, 0.028566038415654433]\n",
            "num_epochs 12\n",
            "len of train_loss is: 12\n",
            "Loss Difference: 0.002257026823019346\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.026626173843664724\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566, 0.03667718176613776, 0.03348518626855716, 0.03082306523867378, 0.028566038415654433, 0.026626173843664724]\n",
            "num_epochs 13\n",
            "len of train_loss is: 13\n",
            "Loss Difference: 0.0019398645719897085\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.02493962250406359\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566, 0.03667718176613776, 0.03348518626855716, 0.03082306523867378, 0.028566038415654433, 0.026626173843664724, 0.02493962250406359]\n",
            "num_epochs 14\n",
            "len of train_loss is: 14\n",
            "Loss Difference: 0.0016865513396011333\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.02345883556380749\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566, 0.03667718176613776, 0.03348518626855716, 0.03082306523867378, 0.028566038415654433, 0.026626173843664724, 0.02493962250406359, 0.02345883556380749]\n",
            "num_epochs 15\n",
            "len of train_loss is: 15\n",
            "Loss Difference: 0.0014807869402561\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.02214762967527783\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566, 0.03667718176613776, 0.03348518626855716, 0.03082306523867378, 0.028566038415654433, 0.026626173843664724, 0.02493962250406359, 0.02345883556380749, 0.02214762967527783]\n",
            "num_epochs 16\n",
            "len of train_loss is: 16\n",
            "Loss Difference: 0.0013112058885296624\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.02097795190410939\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566, 0.03667718176613776, 0.03348518626855716, 0.03082306523867378, 0.028566038415654433, 0.026626173843664724, 0.02493962250406359, 0.02345883556380749, 0.02214762967527783, 0.02097795190410939]\n",
            "num_epochs 17\n",
            "len of train_loss is: 17\n",
            "Loss Difference: 0.001169677771168439\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.019927696107006242\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566, 0.03667718176613776, 0.03348518626855716, 0.03082306523867378, 0.028566038415654433, 0.026626173843664724, 0.02493962250406359, 0.02345883556380749, 0.02214762967527783, 0.02097795190410939, 0.019927696107006242]\n",
            "num_epochs 18\n",
            "len of train_loss is: 18\n",
            "Loss Difference: 0.0010502557971031473\n",
            "Tollerence is:  0.001\n",
            "Len of Y_pred 1006\n",
            "Len of  y_train 1006\n",
            "loss_tr 0.018979191514232704\n",
            "train_loss array [0.21071039693014837, 0.12789779912888735, 0.09236735360552176, 0.0728411306682425, 0.06044738113186946, 0.05183881727143685, 0.045483684031713684, 0.040582614034499566, 0.03667718176613776, 0.03348518626855716, 0.03082306523867378, 0.028566038415654433, 0.026626173843664724, 0.02493962250406359, 0.02345883556380749, 0.02214762967527783, 0.02097795190410939, 0.019927696107006242, 0.018979191514232704]\n",
            "num_epochs 19\n",
            "len of train_loss is: 19\n",
            "Loss Difference: 0.0009485045927735379\n",
            "Tollerence is:  0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYy8iqhrAne2"
      },
      "source": [
        "### 8. Custom function to make predictions using logistic regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2gy5NvAlC_6"
      },
      "source": [
        "def predict(w,b, X):\n",
        "    '''function to predict label given weights, bias and standardized data'''\n",
        "    t_predictions=[]\n",
        "    for x1 in X:\n",
        "      tst_pred=custom_sigmoid(np.dot(x1.toarray(),w.T)+b)\n",
        "      print(\"prediction is \",tst_pred)\n",
        "      t_predictions.append(tst_pred) \n",
        "    print(\"All predicitons from pred function before return:\",t_predictions)\n",
        "    predictions=np.zeros(X.shape[0])\n",
        "    for i,ele in enumerate(t_predictions):\n",
        "      if (ele<=0.5):\n",
        "        predictions[i]=0\n",
        "      else:\n",
        "        predictions[i]=1\n",
        "    \n",
        "    print(\"Numpy predicitons converted\",predictions)\n",
        "    return predictions #it should be a numpy array"
      ],
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0_T2jGhbndD",
        "outputId": "d3f2bd40-32d9-4abc-de02-75fadb478e74"
      },
      "source": [
        "test_vectors_stand.shape[0]"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 271
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY3E_yv2OGiD"
      },
      "source": [
        "### Grader Function - 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKYawSBtlRD7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e51c2e2c-4b66-4026-e5e3-717d0c245a30"
      },
      "source": [
        "def grader_predict():\n",
        "  ''' grader to check the test accuracy'''\n",
        "  # w,b,_,_ = custom_train(train_vectors_stand, train_category.values, 0.0001,0.0001,0.001)\n",
        "  # print(\"After Train in Predict\")\n",
        "  print(\"one\")\n",
        "  test_preds= predict(w,b,test_vectors_stand)\n",
        "  print(\"two\",test_preds)\n",
        "  print(\"actuals\",test_category)\n",
        "  test_accuracy= (np.sum(test_category==test_preds)/len(test_preds))*100\n",
        "  print(\"Three\",test_accuracy)\n",
        "  if(test_accuracy>=90):\n",
        "    print(\"Success!\")\n",
        "  else:\n",
        "    print(\"Failed! \\n Test accuracy = \", test_accuracy)\n",
        "  return\n",
        "  \n",
        "grader_predict()"
      ],
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "one\n",
            "prediction is  0.8773279387770337\n",
            "prediction is  0.004810998870344215\n",
            "prediction is  0.9905644999421929\n",
            "prediction is  0.10019583199955427\n",
            "prediction is  0.9093202344543\n",
            "prediction is  0.9216191717567307\n",
            "prediction is  0.03211623603342575\n",
            "prediction is  0.9958631266380925\n",
            "prediction is  0.06770579830075057\n",
            "prediction is  0.9768437525097754\n",
            "prediction is  0.029299652342819303\n",
            "All predicitons from pred function before return: [0.8773279387770337, 0.004810998870344215, 0.9905644999421929, 0.10019583199955427, 0.9093202344543, 0.9216191717567307, 0.03211623603342575, 0.9958631266380925, 0.06770579830075057, 0.9768437525097754, 0.029299652342819303]\n",
            "Numpy predicitons converted [1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0.]\n",
            "two [1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0.]\n",
            "actuals 788    1\n",
            "532    0\n",
            "321    1\n",
            "687    0\n",
            "62     1\n",
            "761    1\n",
            "883    0\n",
            "347    1\n",
            "199    0\n",
            "301    1\n",
            "195    0\n",
            "Name: category, dtype: int64\n",
            "Three 100.0\n",
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}